--- Beginning of Page 1 ---
Line 1, 19:01:32: Dataset Creation
Line 2, 19:01:32: To address the challenge of acquiring real depositions for fine-tuning Mistral-7B-Instruct-v0.2 for
Line 3, 19:01:32: automated summarization, we can leverage large language models (LLMs) like Gemini or ChatGPT to
Line 4, 19:01:32: generate synthetic depositions.
Line 5, 19:01:32: Here's a step-by-step guide:
Line 6, 19:01:32: 1. Generating Synthetic Depositions:
Line 7, 19:01:32:  Prompt the LLM: Provide clear instructions to the LLM, including the type of deposition (e.g.,
Line 8, 19:01:32: accident, theft) and any relevant details (e.g., parties involved, timeframes).
Line 9, 19:01:32:  Iterative Refinement: Start with a basic prompt and progressively refine the deposition by
Line 10, 19:01:32: prompting the LLM to "extend the deposition based on the previous response."
Line 11, 19:01:32:  Summarization: Once satisfied with the deposition, prompt the LLM to "generate a summary of
Line 12, 19:01:32: the deposition."
Line 13, 19:01:32: 2. Building the Dataset:
Line 14, 19:01:32:  Repeat the process: Generate multiple deposition-summary pairs by repeating steps ensuring
Line 15, 19:01:32: diverse scenarios in the dataset.
Line 16, 19:01:32: Building a Compatible Dataset for Fine-Tuning
Line 17, 19:01:32: After generating synthetic depositions and summaries using an LLM, we need to structure the data
Line 18, 19:01:32: appropriately for fine-tuning Mistral-7B-Instruct-v0.2. Here's how:
Line 19, 19:01:32: 1. Create a CSV File:
Line 20, 19:01:32: a. Open a spreadsheet editor or text editor.
Line 21, 19:01:32: b. Create two columns titled "Deposition" and "Summary."
Line 22, 19:01:32: c. Fill each row with a corresponding deposition-summary pair.
Line 23, 19:01:32: d. Use the pipe symbol ("|") to separate the deposition from its summary within each row.
Line 24, 19:01:32: This adheres to standard formatting for many data processing tasks.
Line 25, 19:01:32: 2. Understand Mistral AI's Instruction Format:
Line 26, 19:01:32: a. Mistral AI requires a specific format to fine-tune their model effectively. This involves
Line 27, 19:01:32: surrounding your instructions (in our case, the depositions) with special tokens:
Line 28, 19:01:32: i. [INST]: indicates the beginning of an instruction.
Line 29, 19:01:32: ii. [/INST]: marks the end of an instruction
Line 30, 19:01:32: Example:
Line 31, 19:01:32: text = "<s>[INST] What is your favourite condiment? [/INST]"
Line 32, 19:01:32: "Well, I'm quite partial to a good squeeze of fresh lemon juice.
Line 33, 19:01:32: It adds just the right amount of zesty flavour to whatever I'm
Line 34, 19:01:32: cooking up in the kitchen!</s> "
--- End of Page 1 ---

--- Beginning of Page 2 ---
Line 1, 19:01:32: Our dataset will consist of three main elements:
Line 2, 19:01:32:  Deposition: This field contains the raw text of the legal statement.
Line 3, 19:01:32:  Summary: This field holds the condensed version of the corresponding deposition.
Line 4, 19:01:32:  Text: This crucial field is specifically formatted for the fine-tuning process, as it guides the model
Line 5, 19:01:32: during training. Its structure will be further explained later in this document.
Line 6, 19:01:32: Key Points and Logic:
Line 7, 19:01:32: JSONL Format: Each row in the JSON Lines file will be a self-contained JSON object, representing a single
Line 8, 19:01:32: training example for fine-tuning the Mistral-7B-Instruct-v0.2 model.
Line 9, 19:01:32: Data Elements: Each row will have the following structure:
Line 10, 19:01:32: JSON
Line 11, 19:01:32: {
Line 12, 19:01:32: "deposition": "(the text of the deposition)",
Line 13, 19:01:32: "summary": "(the corresponding summary)",
Line 14, 19:01:32: "text": "(the formatted text ready for Mistral-7B-Instruct-v0.2)"
Line 15, 19:01:32: }
Line 16, 19:01:32: We will utilize a function responsible for concatenating the instructions, deposition, summary, and
Line 17, 19:01:32: special tokens according to Mistral AI's format. The output will populate the text key of each row.
Line 18, 19:01:32: Example:
Line 19, 19:01:32: Let's assume you have the following dummy deposition-summary pair in your CSV file:
Line 20, 19:01:32:  Deposition: "The witness saw a blue car speeding through the intersection and hitting a
Line 21, 19:01:32: pedestrian on the crosswalk."
Line 22, 19:01:32:  Summary: "Blue car runs intersection, hits pedestrian."
Line 23, 19:01:32: JSONL Row: After formatting by the code, the corresponding row in your JSONL file would look like:
Line 24, 19:01:32: JSON
Line 25, 19:01:32: {
Line 26, 19:01:32: "deposition": "The witness saw a blue car speeding through the
Line 27, 19:01:32: intersection and hitting a pedestrian on the crosswalk.",
Line 28, 19:01:32: "summary": "Blue car runs intersection, hits pedestrian.",
Line 29, 19:01:32: "text": "<s>[INST] Provide a summary of the following: The witness saw a
Line 30, 19:01:32: blue car speeding through the intersection and hitting a pedestrian on the
Line 31, 19:01:32: crosswalk. [/INST] \n Blue car runs intersection, hits pedestrian. </s>"
Line 32, 19:01:32: }
Line 33, 19:01:32: Explanation:
Line 34, 19:01:32: deposition and summary: These fields contain the raw deposition and summary text, respectively.
Line 35, 19:01:32: text: This field contains the deposition, your instruction ("Provide a summary of the following"), and the
Line 36, 19:01:32: summary, surrounded by the [INST], [/INST] tokens. This is what Mistral AI's model will be trained on.
--- End of Page 2 ---

--- Beginning of Page 3 ---
Line 1, 19:01:32: How the model uses this:
Line 2, 19:01:32: Mistral-7B-Instruct-v0.2 is designed to learn from examples. When you feed it a JSONL file containing
Line 3, 19:01:32: examples like this, it internalizes the pattern:
Line 4, 19:01:32: [INST] indicates the start of a task
Line 5, 19:01:32: Text following [INST] is the input to process
Line 6, 19:01:32: Text following \n is the correct solution
Line 7, 19:01:32: [/INST] signals the end of the task
Line 8, 19:01:32: The samples are divided into 3 categories of the dataset as explained below:
Line 9, 19:01:32: Training Dataset
Line 10, 19:01:32:  Purpose: The training dataset is used to train the machine learning model. During training, the
Line 11, 19:01:32: model learns patterns and relationships in the data, optimizing its parameters to minimize the
Line 12, 19:01:32: error between predicted and actual values.
Line 13, 19:01:32:  Composition: The training dataset typically constitutes the largest portion of the available data.
Line 14, 19:01:32:  Usage: The model iteratively processes batches of data from the training dataset, updating its
Line 15, 19:01:32: parameters through techniques like gradient descent or backpropagation.
Line 16, 19:01:32:  Outcome: After training, the model should be capable of making predictions on new, unseen
Line 17, 19:01:32: data.
Line 18, 19:01:32: Validation Dataset
Line 19, 19:01:32:  Purpose: The validation dataset is used to tune hyperparameters and assess the model's
Line 20, 19:01:32: performance during training.
Line 21, 19:01:32:  Composition: The validation dataset is a separate portion of the dataset that is not used for
Line 22, 19:01:32: training. It is often a subset of the overall dataset.
Line 23, 19:01:32:  Usage: During the training process, the model's performance on the validation dataset is
Line 24, 19:01:32: periodically evaluated. This evaluation helps identify potential overfitting and guides
Line 25, 19:01:32: adjustments to the model's architecture or hyperparameters.
Line 26, 19:01:32:  Outcome: By monitoring the model's performance on the validation dataset, practitioners can
Line 27, 19:01:32: make informed decisions to improve the model's generalization capabilities.
Line 28, 19:01:32: Test Dataset
Line 29, 19:01:32:  Purpose: The test dataset is used to evaluate the final performance of the trained model.
Line 30, 19:01:32:  Composition: Like the validation dataset, the test dataset is separate from both the training and
Line 31, 19:01:32: validation datasets.
Line 32, 19:01:32:  Usage: Once the model has been trained and tuned using the training and validation datasets, it
Line 33, 19:01:32: is evaluated on the test dataset. The test dataset provides an unbiased assessment of the
Line 34, 19:01:32: model's ability to generalize to unseen data.
Line 35, 19:01:32:  Outcome: The performance metrics obtained from the test dataset reflect the model's
Line 36, 19:01:32: performance in real-world scenarios. These metrics guide decisions regarding model
Line 37, 19:01:32: deployment and further improvements.
--- End of Page 3 ---

--- Beginning of Page 1 ---
Line 1, 19:20:49: Dataset Creation
Line 2, 19:20:49: To address the challenge of acquiring real depositions for fine-tuning Mistral-7B-Instruct-v0.2 for
Line 3, 19:20:49: automated summarization, we can leverage large language models (LLMs) like Gemini or ChatGPT to
Line 4, 19:20:49: generate synthetic depositions.
Line 5, 19:20:49: Here's a step-by-step guide:
Line 6, 19:20:49: 1. Generating Synthetic Depositions:
Line 7, 19:20:49:  Prompt the LLM: Provide clear instructions to the LLM, including the type of deposition (e.g.,
Line 8, 19:20:49: accident, theft) and any relevant details (e.g., parties involved, timeframes).
Line 9, 19:20:49:  Iterative Refinement: Start with a basic prompt and progressively refine the deposition by
Line 10, 19:20:49: prompting the LLM to "extend the deposition based on the previous response."
Line 11, 19:20:49:  Summarization: Once satisfied with the deposition, prompt the LLM to "generate a summary of
Line 12, 19:20:49: the deposition."
Line 13, 19:20:49: 2. Building the Dataset:
Line 14, 19:20:49:  Repeat the process: Generate multiple deposition-summary pairs by repeating steps ensuring
Line 15, 19:20:49: diverse scenarios in the dataset.
Line 16, 19:20:49: Building a Compatible Dataset for Fine-Tuning
Line 17, 19:20:49: After generating synthetic depositions and summaries using an LLM, we need to structure the data
Line 18, 19:20:49: appropriately for fine-tuning Mistral-7B-Instruct-v0.2. Here's how:
Line 19, 19:20:49: 1. Create a CSV File:
Line 20, 19:20:49: a. Open a spreadsheet editor or text editor.
Line 21, 19:20:49: b. Create two columns titled "Deposition" and "Summary."
Line 22, 19:20:49: c. Fill each row with a corresponding deposition-summary pair.
Line 23, 19:20:49: d. Use the pipe symbol ("|") to separate the deposition from its summary within each row.
Line 24, 19:20:49: This adheres to standard formatting for many data processing tasks.
Line 25, 19:20:49: 2. Understand Mistral AI's Instruction Format:
Line 26, 19:20:49: a. Mistral AI requires a specific format to fine-tune their model effectively. This involves
Line 27, 19:20:49: surrounding your instructions (in our case, the depositions) with special tokens:
Line 28, 19:20:49: i. [INST]: indicates the beginning of an instruction.
Line 29, 19:20:49: ii. [/INST]: marks the end of an instruction
Line 30, 19:20:49: Example:
Line 31, 19:20:49: text = "<s>[INST] What is your favourite condiment? [/INST]"
Line 32, 19:20:49: "Well, I'm quite partial to a good squeeze of fresh lemon juice.
Line 33, 19:20:49: It adds just the right amount of zesty flavour to whatever I'm
Line 34, 19:20:49: cooking up in the kitchen!</s> "
--- End of Page 1 ---

--- Beginning of Page 2 ---
Line 1, 19:20:49: Our dataset will consist of three main elements:
Line 2, 19:20:49:  Deposition: This field contains the raw text of the legal statement.
Line 3, 19:20:49:  Summary: This field holds the condensed version of the corresponding deposition.
Line 4, 19:20:49:  Text: This crucial field is specifically formatted for the fine-tuning process, as it guides the model
Line 5, 19:20:49: during training. Its structure will be further explained later in this document.
Line 6, 19:20:49: Key Points and Logic:
Line 7, 19:20:49: JSONL Format: Each row in the JSON Lines file will be a self-contained JSON object, representing a single
Line 8, 19:20:49: training example for fine-tuning the Mistral-7B-Instruct-v0.2 model.
Line 9, 19:20:49: Data Elements: Each row will have the following structure:
Line 10, 19:20:49: JSON
Line 11, 19:20:49: {
Line 12, 19:20:49: "deposition": "(the text of the deposition)",
Line 13, 19:20:49: "summary": "(the corresponding summary)",
Line 14, 19:20:49: "text": "(the formatted text ready for Mistral-7B-Instruct-v0.2)"
Line 15, 19:20:49: }
Line 16, 19:20:49: We will utilize a function responsible for concatenating the instructions, deposition, summary, and
Line 17, 19:20:49: special tokens according to Mistral AI's format. The output will populate the text key of each row.
Line 18, 19:20:49: Example:
Line 19, 19:20:49: Let's assume you have the following dummy deposition-summary pair in your CSV file:
Line 20, 19:20:49:  Deposition: "The witness saw a blue car speeding through the intersection and hitting a
Line 21, 19:20:49: pedestrian on the crosswalk."
Line 22, 19:20:49:  Summary: "Blue car runs intersection, hits pedestrian."
Line 23, 19:20:49: JSONL Row: After formatting by the code, the corresponding row in your JSONL file would look like:
Line 24, 19:20:49: JSON
Line 25, 19:20:49: {
Line 26, 19:20:49: "deposition": "The witness saw a blue car speeding through the
Line 27, 19:20:49: intersection and hitting a pedestrian on the crosswalk.",
Line 28, 19:20:49: "summary": "Blue car runs intersection, hits pedestrian.",
Line 29, 19:20:49: "text": "<s>[INST] Provide a summary of the following: The witness saw a
Line 30, 19:20:49: blue car speeding through the intersection and hitting a pedestrian on the
Line 31, 19:20:49: crosswalk. [/INST] \n Blue car runs intersection, hits pedestrian. </s>"
Line 32, 19:20:49: }
Line 33, 19:20:49: Explanation:
Line 34, 19:20:49: deposition and summary: These fields contain the raw deposition and summary text, respectively.
Line 35, 19:20:49: text: This field contains the deposition, your instruction ("Provide a summary of the following"), and the
Line 36, 19:20:49: summary, surrounded by the [INST], [/INST] tokens. This is what Mistral AI's model will be trained on.
--- End of Page 2 ---

--- Beginning of Page 3 ---
Line 1, 19:20:49: How the model uses this:
Line 2, 19:20:49: Mistral-7B-Instruct-v0.2 is designed to learn from examples. When you feed it a JSONL file containing
Line 3, 19:20:49: examples like this, it internalizes the pattern:
Line 4, 19:20:49: [INST] indicates the start of a task
Line 5, 19:20:49: Text following [INST] is the input to process
Line 6, 19:20:49: Text following \n is the correct solution
Line 7, 19:20:49: [/INST] signals the end of the task
Line 8, 19:20:49: The samples are divided into 3 categories of the dataset as explained below:
Line 9, 19:20:49: Training Dataset
Line 10, 19:20:49:  Purpose: The training dataset is used to train the machine learning model. During training, the
Line 11, 19:20:49: model learns patterns and relationships in the data, optimizing its parameters to minimize the
Line 12, 19:20:49: error between predicted and actual values.
Line 13, 19:20:49:  Composition: The training dataset typically constitutes the largest portion of the available data.
Line 14, 19:20:49:  Usage: The model iteratively processes batches of data from the training dataset, updating its
Line 15, 19:20:49: parameters through techniques like gradient descent or backpropagation.
Line 16, 19:20:49:  Outcome: After training, the model should be capable of making predictions on new, unseen
Line 17, 19:20:49: data.
Line 18, 19:20:49: Validation Dataset
Line 19, 19:20:49:  Purpose: The validation dataset is used to tune hyperparameters and assess the model's
Line 20, 19:20:49: performance during training.
Line 21, 19:20:49:  Composition: The validation dataset is a separate portion of the dataset that is not used for
Line 22, 19:20:49: training. It is often a subset of the overall dataset.
Line 23, 19:20:49:  Usage: During the training process, the model's performance on the validation dataset is
Line 24, 19:20:49: periodically evaluated. This evaluation helps identify potential overfitting and guides
Line 25, 19:20:49: adjustments to the model's architecture or hyperparameters.
Line 26, 19:20:49:  Outcome: By monitoring the model's performance on the validation dataset, practitioners can
Line 27, 19:20:49: make informed decisions to improve the model's generalization capabilities.
Line 28, 19:20:49: Test Dataset
Line 29, 19:20:49:  Purpose: The test dataset is used to evaluate the final performance of the trained model.
Line 30, 19:20:49:  Composition: Like the validation dataset, the test dataset is separate from both the training and
Line 31, 19:20:49: validation datasets.
Line 32, 19:20:49:  Usage: Once the model has been trained and tuned using the training and validation datasets, it
Line 33, 19:20:49: is evaluated on the test dataset. The test dataset provides an unbiased assessment of the
Line 34, 19:20:49: model's ability to generalize to unseen data.
Line 35, 19:20:49:  Outcome: The performance metrics obtained from the test dataset reflect the model's
Line 36, 19:20:49: performance in real-world scenarios. These metrics guide decisions regarding model
Line 37, 19:20:49: deployment and further improvements.
--- End of Page 3 ---

