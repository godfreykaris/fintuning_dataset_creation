{"deposition": "--- Beginning of Page 1 ---\nLine 1, 16:07:17: Dataset Creation\nLine 2, 16:07:17: To address the challenge of acquiring real depositions for fine-tuning Mistral-7B-Instruct-v0.2 for\nLine 3, 16:07:17: automated summarization, we can leverage large language models (LLMs) like Gemini or ChatGPT to\nLine 4, 16:07:17: generate synthetic depositions.\nLine 5, 16:07:17: Here's a step-by-step guide:\nLine 6, 16:07:17: 1. Generating Synthetic Depositions:\nLine 7, 16:07:17: \uf0b7 Prompt the LLM: Provide clear instructions to the LLM, including the type of deposition (e.g.,\nLine 8, 16:07:17: accident, theft) and any relevant details (e.g., parties involved, timeframes).\nLine 9, 16:07:17: \uf0b7 Iterative Refinement: Start with a basic prompt and progressively refine the deposition by\nLine 10, 16:07:17: prompting the LLM to \"extend the deposition based on the previous response.\"\nLine 11, 16:07:17: \uf0b7 Summarization: Once satisfied with the deposition, prompt the LLM to \"generate a summary of\nLine 12, 16:07:17: the deposition.\"\nLine 13, 16:07:17: 2. Building the Dataset:\nLine 14, 16:07:17: \uf0b7 Repeat the process: Generate multiple deposition-summary pairs by repeating steps ensuring\nLine 15, 16:07:17: diverse scenarios in the dataset.\nLine 16, 16:07:17: Building a Compatible Dataset for Fine-Tuning\nLine 17, 16:07:17: After generating synthetic depositions and summaries using an LLM, we need to structure the data\nLine 18, 16:07:17: appropriately for fine-tuning Mistral-7B-Instruct-v0.2. Here's how:\nLine 19, 16:07:17: 1. Create a CSV File:\nLine 20, 16:07:17: a. Open a spreadsheet editor or text editor.\nLine 21, 16:07:17: b. Create two columns titled \"Deposition\" and \"Summary.\"\nLine 22, 16:07:17: c. Fill each row with a corresponding deposition-summary pair.\nLine 23, 16:07:17: d. Use the pipe symbol (\"|\") to separate the deposition from its summary within each row.\nLine 24, 16:07:17: This adheres to standard formatting for many data processing tasks.\nLine 25, 16:07:17: 2. Understand Mistral AI's Instruction Format:\nLine 26, 16:07:17: a. Mistral AI requires a specific format to fine-tune their model effectively. This involves\nLine 27, 16:07:17: surrounding your instructions (in our case, the depositions) with special tokens:\nLine 28, 16:07:17: i. [INST]: indicates the beginning of an instruction.\nLine 29, 16:07:17: ii. [/INST]: marks the end of an instruction\nLine 30, 16:07:17: Example:\nLine 31, 16:07:17: text = \"<s>[INST] What is your favourite condiment? [/INST]\"\nLine 32, 16:07:17: \"Well, I'm quite partial to a good squeeze of fresh lemon juice.\nLine 33, 16:07:17: It adds just the right amount of zesty flavour to whatever I'm\nLine 34, 16:07:17: cooking up in the kitchen!</s> \"\n--- End of Page 1 ---\n\n--- Beginning of Page 2 ---\nLine 35, 16:07:17: Our dataset will consist of three main elements:\nLine 36, 16:07:17: \uf0b7 Deposition: This field contains the raw text of the legal statement.\nLine 37, 16:07:17: \uf0b7 Summary: This field holds the condensed version of the corresponding deposition.\nLine 38, 16:07:17: \uf0b7 Text: This crucial field is specifically formatted for the fine-tuning process, as it guides the model\nLine 39, 16:07:17: during training. Its structure will be further explained later in this document.\nLine 40, 16:07:17: Key Points and Logic:\nLine 41, 16:07:17: JSONL Format: Each row in the JSON Lines file will be a self-contained JSON object, representing a single\nLine 42, 16:07:17: training example for fine-tuning the Mistral-7B-Instruct-v0.2 model.\nLine 43, 16:07:17: Data Elements: Each row will have the following structure:\nLine 44, 16:07:17: JSON\nLine 45, 16:07:17: {\nLine 46, 16:07:17: \"deposition\": \"(the text of the deposition)\",\nLine 47, 16:07:17: \"summary\": \"(the corresponding summary)\",\nLine 48, 16:07:17: \"text\": \"(the formatted text ready for Mistral-7B-Instruct-v0.2)\"\nLine 49, 16:07:17: }\nLine 50, 16:07:17: We will utilize a function responsible for concatenating the instructions, deposition, summary, and\nLine 51, 16:07:17: special tokens according to Mistral AI's format. The output will populate the text key of each row.\nLine 52, 16:07:17: Example:\nLine 53, 16:07:17: Let's assume you have the following dummy deposition-summary pair in your CSV file:\nLine 54, 16:07:17: \uf0b7 Deposition: \"The witness saw a blue car speeding through the intersection and hitting a\nLine 55, 16:07:17: pedestrian on the crosswalk.\"\nLine 56, 16:07:17: \uf0b7 Summary: \"Blue car runs intersection, hits pedestrian.\"\nLine 57, 16:07:17: JSONL Row: After formatting by the code, the corresponding row in your JSONL file would look like:\nLine 58, 16:07:17: JSON\nLine 59, 16:07:17: {\nLine 60, 16:07:17: \"deposition\": \"The witness saw a blue car speeding through the\nLine 61, 16:07:17: intersection and hitting a pedestrian on the crosswalk.\",\nLine 62, 16:07:17: \"summary\": \"Blue car runs intersection, hits pedestrian.\",\nLine 63, 16:07:17: \"text\": \"<s>[INST] Provide a summary of the following: The witness saw a\nLine 64, 16:07:17: blue car speeding through the intersection and hitting a pedestrian on the\nLine 65, 16:07:17: crosswalk. [/INST] \\n Blue car runs intersection, hits pedestrian. </s>\"\nLine 66, 16:07:17: }\nLine 67, 16:07:17: Explanation:\nLine 68, 16:07:17: deposition and summary: These fields contain the raw deposition and summary text, respectively.\nLine 69, 16:07:17: text: This field contains the deposition, your instruction (\"Provide a summary of the following\"), and the\nLine 70, 16:07:17: summary, surrounded by the [INST], [/INST] tokens. This is what Mistral AI's model will be trained on.\n--- End of Page 2 ---\n\n--- Beginning of Page 3 ---\nLine 71, 16:07:17: How the model uses this:\nLine 72, 16:07:17: Mistral-7B-Instruct-v0.2 is designed to learn from examples. When you feed it a JSONL file containing\nLine 73, 16:07:17: examples like this, it internalizes the pattern:\nLine 74, 16:07:17: [INST] indicates the start of a task\nLine 75, 16:07:17: Text following [INST] is the input to process\nLine 76, 16:07:17: Text following \\n is the correct solution\nLine 77, 16:07:17: [/INST] signals the end of the task\nLine 78, 16:07:17: The samples are divided into 3 categories of the dataset as explained below:\nLine 79, 16:07:17: Training Dataset\nLine 80, 16:07:17: \uf0b7 Purpose: The training dataset is used to train the machine learning model. During training, the\nLine 81, 16:07:17: model learns patterns and relationships in the data, optimizing its parameters to minimize the\nLine 82, 16:07:17: error between predicted and actual values.\nLine 83, 16:07:17: \uf0b7 Composition: The training dataset typically constitutes the largest portion of the available data.\nLine 84, 16:07:17: \uf0b7 Usage: The model iteratively processes batches of data from the training dataset, updating its\nLine 85, 16:07:17: parameters through techniques like gradient descent or backpropagation.\nLine 86, 16:07:17: \uf0b7 Outcome: After training, the model should be capable of making predictions on new, unseen\nLine 87, 16:07:17: data.\nLine 88, 16:07:17: Validation Dataset\nLine 89, 16:07:17: \uf0b7 Purpose: The validation dataset is used to tune hyperparameters and assess the model's\nLine 90, 16:07:17: performance during training.\nLine 91, 16:07:17: \uf0b7 Composition: The validation dataset is a separate portion of the dataset that is not used for\nLine 92, 16:07:17: training. It is often a subset of the overall dataset.\nLine 93, 16:07:17: \uf0b7 Usage: During the training process, the model's performance on the validation dataset is\nLine 94, 16:07:17: periodically evaluated. This evaluation helps identify potential overfitting and guides\nLine 95, 16:07:17: adjustments to the model's architecture or hyperparameters.\nLine 96, 16:07:17: \uf0b7 Outcome: By monitoring the model's performance on the validation dataset, practitioners can\nLine 97, 16:07:17: make informed decisions to improve the model's generalization capabilities.\nLine 98, 16:07:17: Test Dataset\nLine 99, 16:07:17: \uf0b7 Purpose: The test dataset is used to evaluate the final performance of the trained model.\nLine 100, 16:07:17: \uf0b7 Composition: Like the validation dataset, the test dataset is separate from both the training and\nLine 101, 16:07:17: validation datasets.\nLine 102, 16:07:17: \uf0b7 Usage: Once the model has been trained and tuned using the training and validation datasets, it\nLine 103, 16:07:17: is evaluated on the test dataset. The test dataset provides an unbiased assessment of the\nLine 104, 16:07:17: model's ability to generalize to unseen data.\nLine 105, 16:07:17: \uf0b7 Outcome: The performance metrics obtained from the test dataset reflect the model's\nLine 106, 16:07:17: performance in real-world scenarios. These metrics guide decisions regarding model\nLine 107, 16:07:17: deployment and further improvements.\n--- End of Page 3 ---", "summary": "--- Beginning of Page 1 ---\nLine 1, 16:07:20: Dummy Summary two\n--- End of Page 1 ---", "text": "<s>[INST] Provide a summary of the following : --- Beginning of Page 1 ---\nLine 1, 16:07:17: Dataset Creation\nLine 2, 16:07:17: To address the challenge of acquiring real depositions for fine-tuning Mistral-7B-Instruct-v0.2 for\nLine 3, 16:07:17: automated summarization, we can leverage large language models (LLMs) like Gemini or ChatGPT to\nLine 4, 16:07:17: generate synthetic depositions.\nLine 5, 16:07:17: Here's a step-by-step guide:\nLine 6, 16:07:17: 1. Generating Synthetic Depositions:\nLine 7, 16:07:17: \uf0b7 Prompt the LLM: Provide clear instructions to the LLM, including the type of deposition (e.g.,\nLine 8, 16:07:17: accident, theft) and any relevant details (e.g., parties involved, timeframes).\nLine 9, 16:07:17: \uf0b7 Iterative Refinement: Start with a basic prompt and progressively refine the deposition by\nLine 10, 16:07:17: prompting the LLM to \"extend the deposition based on the previous response.\"\nLine 11, 16:07:17: \uf0b7 Summarization: Once satisfied with the deposition, prompt the LLM to \"generate a summary of\nLine 12, 16:07:17: the deposition.\"\nLine 13, 16:07:17: 2. Building the Dataset:\nLine 14, 16:07:17: \uf0b7 Repeat the process: Generate multiple deposition-summary pairs by repeating steps ensuring\nLine 15, 16:07:17: diverse scenarios in the dataset.\nLine 16, 16:07:17: Building a Compatible Dataset for Fine-Tuning\nLine 17, 16:07:17: After generating synthetic depositions and summaries using an LLM, we need to structure the data\nLine 18, 16:07:17: appropriately for fine-tuning Mistral-7B-Instruct-v0.2. Here's how:\nLine 19, 16:07:17: 1. Create a CSV File:\nLine 20, 16:07:17: a. Open a spreadsheet editor or text editor.\nLine 21, 16:07:17: b. Create two columns titled \"Deposition\" and \"Summary.\"\nLine 22, 16:07:17: c. Fill each row with a corresponding deposition-summary pair.\nLine 23, 16:07:17: d. Use the pipe symbol (\"|\") to separate the deposition from its summary within each row.\nLine 24, 16:07:17: This adheres to standard formatting for many data processing tasks.\nLine 25, 16:07:17: 2. Understand Mistral AI's Instruction Format:\nLine 26, 16:07:17: a. Mistral AI requires a specific format to fine-tune their model effectively. This involves\nLine 27, 16:07:17: surrounding your instructions (in our case, the depositions) with special tokens:\nLine 28, 16:07:17: i. [INST]: indicates the beginning of an instruction.\nLine 29, 16:07:17: ii. [/INST]: marks the end of an instruction\nLine 30, 16:07:17: Example:\nLine 31, 16:07:17: text = \"<s>[INST] What is your favourite condiment? [/INST]\"\nLine 32, 16:07:17: \"Well, I'm quite partial to a good squeeze of fresh lemon juice.\nLine 33, 16:07:17: It adds just the right amount of zesty flavour to whatever I'm\nLine 34, 16:07:17: cooking up in the kitchen!</s> \"\n--- End of Page 1 ---\n\n--- Beginning of Page 2 ---\nLine 35, 16:07:17: Our dataset will consist of three main elements:\nLine 36, 16:07:17: \uf0b7 Deposition: This field contains the raw text of the legal statement.\nLine 37, 16:07:17: \uf0b7 Summary: This field holds the condensed version of the corresponding deposition.\nLine 38, 16:07:17: \uf0b7 Text: This crucial field is specifically formatted for the fine-tuning process, as it guides the model\nLine 39, 16:07:17: during training. Its structure will be further explained later in this document.\nLine 40, 16:07:17: Key Points and Logic:\nLine 41, 16:07:17: JSONL Format: Each row in the JSON Lines file will be a self-contained JSON object, representing a single\nLine 42, 16:07:17: training example for fine-tuning the Mistral-7B-Instruct-v0.2 model.\nLine 43, 16:07:17: Data Elements: Each row will have the following structure:\nLine 44, 16:07:17: JSON\nLine 45, 16:07:17: {\nLine 46, 16:07:17: \"deposition\": \"(the text of the deposition)\",\nLine 47, 16:07:17: \"summary\": \"(the corresponding summary)\",\nLine 48, 16:07:17: \"text\": \"(the formatted text ready for Mistral-7B-Instruct-v0.2)\"\nLine 49, 16:07:17: }\nLine 50, 16:07:17: We will utilize a function responsible for concatenating the instructions, deposition, summary, and\nLine 51, 16:07:17: special tokens according to Mistral AI's format. The output will populate the text key of each row.\nLine 52, 16:07:17: Example:\nLine 53, 16:07:17: Let's assume you have the following dummy deposition-summary pair in your CSV file:\nLine 54, 16:07:17: \uf0b7 Deposition: \"The witness saw a blue car speeding through the intersection and hitting a\nLine 55, 16:07:17: pedestrian on the crosswalk.\"\nLine 56, 16:07:17: \uf0b7 Summary: \"Blue car runs intersection, hits pedestrian.\"\nLine 57, 16:07:17: JSONL Row: After formatting by the code, the corresponding row in your JSONL file would look like:\nLine 58, 16:07:17: JSON\nLine 59, 16:07:17: {\nLine 60, 16:07:17: \"deposition\": \"The witness saw a blue car speeding through the\nLine 61, 16:07:17: intersection and hitting a pedestrian on the crosswalk.\",\nLine 62, 16:07:17: \"summary\": \"Blue car runs intersection, hits pedestrian.\",\nLine 63, 16:07:17: \"text\": \"<s>[INST] Provide a summary of the following: The witness saw a\nLine 64, 16:07:17: blue car speeding through the intersection and hitting a pedestrian on the\nLine 65, 16:07:17: crosswalk. [/INST] \\n Blue car runs intersection, hits pedestrian. </s>\"\nLine 66, 16:07:17: }\nLine 67, 16:07:17: Explanation:\nLine 68, 16:07:17: deposition and summary: These fields contain the raw deposition and summary text, respectively.\nLine 69, 16:07:17: text: This field contains the deposition, your instruction (\"Provide a summary of the following\"), and the\nLine 70, 16:07:17: summary, surrounded by the [INST], [/INST] tokens. This is what Mistral AI's model will be trained on.\n--- End of Page 2 ---\n\n--- Beginning of Page 3 ---\nLine 71, 16:07:17: How the model uses this:\nLine 72, 16:07:17: Mistral-7B-Instruct-v0.2 is designed to learn from examples. When you feed it a JSONL file containing\nLine 73, 16:07:17: examples like this, it internalizes the pattern:\nLine 74, 16:07:17: [INST] indicates the start of a task\nLine 75, 16:07:17: Text following [INST] is the input to process\nLine 76, 16:07:17: Text following \\n is the correct solution\nLine 77, 16:07:17: [/INST] signals the end of the task\nLine 78, 16:07:17: The samples are divided into 3 categories of the dataset as explained below:\nLine 79, 16:07:17: Training Dataset\nLine 80, 16:07:17: \uf0b7 Purpose: The training dataset is used to train the machine learning model. During training, the\nLine 81, 16:07:17: model learns patterns and relationships in the data, optimizing its parameters to minimize the\nLine 82, 16:07:17: error between predicted and actual values.\nLine 83, 16:07:17: \uf0b7 Composition: The training dataset typically constitutes the largest portion of the available data.\nLine 84, 16:07:17: \uf0b7 Usage: The model iteratively processes batches of data from the training dataset, updating its\nLine 85, 16:07:17: parameters through techniques like gradient descent or backpropagation.\nLine 86, 16:07:17: \uf0b7 Outcome: After training, the model should be capable of making predictions on new, unseen\nLine 87, 16:07:17: data.\nLine 88, 16:07:17: Validation Dataset\nLine 89, 16:07:17: \uf0b7 Purpose: The validation dataset is used to tune hyperparameters and assess the model's\nLine 90, 16:07:17: performance during training.\nLine 91, 16:07:17: \uf0b7 Composition: The validation dataset is a separate portion of the dataset that is not used for\nLine 92, 16:07:17: training. It is often a subset of the overall dataset.\nLine 93, 16:07:17: \uf0b7 Usage: During the training process, the model's performance on the validation dataset is\nLine 94, 16:07:17: periodically evaluated. This evaluation helps identify potential overfitting and guides\nLine 95, 16:07:17: adjustments to the model's architecture or hyperparameters.\nLine 96, 16:07:17: \uf0b7 Outcome: By monitoring the model's performance on the validation dataset, practitioners can\nLine 97, 16:07:17: make informed decisions to improve the model's generalization capabilities.\nLine 98, 16:07:17: Test Dataset\nLine 99, 16:07:17: \uf0b7 Purpose: The test dataset is used to evaluate the final performance of the trained model.\nLine 100, 16:07:17: \uf0b7 Composition: Like the validation dataset, the test dataset is separate from both the training and\nLine 101, 16:07:17: validation datasets.\nLine 102, 16:07:17: \uf0b7 Usage: Once the model has been trained and tuned using the training and validation datasets, it\nLine 103, 16:07:17: is evaluated on the test dataset. The test dataset provides an unbiased assessment of the\nLine 104, 16:07:17: model's ability to generalize to unseen data.\nLine 105, 16:07:17: \uf0b7 Outcome: The performance metrics obtained from the test dataset reflect the model's\nLine 106, 16:07:17: performance in real-world scenarios. These metrics guide decisions regarding model\nLine 107, 16:07:17: deployment and further improvements.\n--- End of Page 3 --- [/INST] \\n --- Beginning of Page 1 ---\nLine 1, 16:07:20: Dummy Summary two\n--- End of Page 1 --- </s>"}
